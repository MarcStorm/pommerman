\section{Discussion}
\label{sec:discussion}
This section will discuss potential issues with the implemented methods that have been utilised during the training and validation of the agents.

\subsection{Reward Function}
% Make reference to book that it is a good idea to use -1,1 reward function
When making reward shaping is it of uttermost importance to keep the final goal in mind. Do we wish to learn the agent to play blow up walls and opponents or do we wish to learn it to play Pommerman? In this case, we obliviously want it to win the game of Pommerman and therefore \emph{\say{we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals. It is thus critical that the rewards we set up
truly indicate what we want accomplished.}}\cite{sutton1998a} By rewarding the agent for completing subgoals like blowing up walls and agents, we do not indicate our true goal. Rewarding the agent for these subgoals could lead to that the agent would blow up walls and opponents even at the cost of losing the game. Sticking to the reward function of $-1$ for losing and $1$ for winning, we tell the agent the we want it to win but not how it should be achieved.\cite{sutton1998a}

\subsection{Lack of Training}
% - Complexity
%   - It takes a lot of correct random actions to get a reward of 1 by killing the enemies
% - Exploration / Exploitation
It is evident from the literature that all the agents that do well have been trained for millions of times. An example of this is \cite{rwightman}'s agent, which has been trained for the same Pommerman environment for approximately 70 millions games. Another one is \cite{kormelink2018exploration} which has been trained for a million games, but in a $7 \times 7$ static classic Bomberman environment rather than a $11 \times 11$ ever changing environment. All of our trained setups showed the same sign of early convergence towards a single action. Our hypothesis is that the setups simple have not had enough time for training and a lot more training would yield for more exploration in the massive search space. This could result in that the agent would be able to take more meaningful actions rather than just sticking to a single action. The reasoning for this hypothesis is that it the amount of correct random actions the agent has to take in order to blow its way out of its initial position to reach the three other agents and to kill them in the process to be awarded with a reward of 1 is very small. Furthermore are the other agents reacting to our agent's movement in an attempt to stay alive and beat our agent. It is quite possible that the search space is simply too large for the agent to ever reach this state of actively beating the three other agents. It seems more likely that the agent could learn to avoid the simple agents and their bombs, as the simple agents are more likely to reach our agent before it is able to learn to place bombs and not killing itself, much like the behaviour of the agent that \cite{rwightman} trained. However, this will require a lot more training in order to verify whether this is true or not for our agent.

\subsection{Networks}

\subsection{Algorithms}
% Maybe talk about the problems with reinforce in the context of Pommerman. 