{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space, state_size):\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.states = torch.zeros(num_steps + 1, num_processes, state_size)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        \n",
    "        action_shape = 1\n",
    "\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "            \n",
    "        self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.observations = self.observations.cuda()\n",
    "        self.states = self.states.cuda()\n",
    "        self.rewards = self.rewards.cuda()\n",
    "        self.value_preds = self.value_preds.cuda()\n",
    "        self.returns = self.returns.cuda()\n",
    "        self.action_log_probs = self.action_log_probs.cuda()\n",
    "        self.actions = self.actions.cuda()\n",
    "        self.masks = self.masks.cuda()\n",
    "\n",
    "    def insert(self, step, current_obs, state, action, action_log_prob, value_pred, reward, mask):\n",
    "        self.observations[step + 1].copy_(current_obs)\n",
    "        self.states[step + 1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.action_log_probs[step].copy_(action_log_prob)\n",
    "        self.value_preds[step].copy_(value_pred)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step + 1].copy_(mask)\n",
    "\n",
    "    def after_update(self):\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value, use_gae, gamma, tau):\n",
    "        self.returns[-1] = next_value\n",
    "        for step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[step] = self.returns[step + 1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
    "                \n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        x = self(x)\n",
    "\n",
    "        probs = F.softmax(x)\n",
    "        if deterministic is False:\n",
    "            action = probs.multinomial()\n",
    "        else:\n",
    "            action = probs.max(1)[1]\n",
    "        return action\n",
    "\n",
    "    def logprobs_and_entropy(self, x, actions):\n",
    "        x = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(x)\n",
    "        probs = F.softmax(x)\n",
    "\n",
    "        action_log_probs = log_probs.gather(1, actions)\n",
    "\n",
    "        dist_entropy = -(log_probs * probs).sum(-1).mean()\n",
    "        return action_log_probs, dist_entropy\n",
    "    \n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        orthogonal(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(32 * 7 * 7, 512)\n",
    "\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "\n",
    "        num_outputs = action_space.n\n",
    "        self.dist = Categorical(512, num_outputs)\n",
    "\n",
    "        self.train() # training mode. Only affects dropout, batchnorm etc\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def act(self, inputs, states, masks, deterministic=False):\n",
    "        value, x, states = self(inputs, states, masks)\n",
    "        action = self.dist.sample(x, deterministic=deterministic)\n",
    "        action_log_probs, dist_entropy = self.dist.logprobs_and_entropy(x, action)\n",
    "        return value, action, action_log_probs, states\n",
    "\n",
    "    def evaluate_actions(self, inputs, states, masks, actions):\n",
    "        value, x, states = self(inputs, states, masks)\n",
    "        action_log_probs, dist_entropy = self.dist.logprobs_and_entropy(x, actions)\n",
    "        return value, action_log_probs, dist_entropy, states\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return 1\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(weights_init)\n",
    "\n",
    "        relu_gain = nn.init.calculate_gain('relu')\n",
    "        self.conv1.weight.data.mul_(relu_gain)\n",
    "        self.conv2.weight.data.mul_(relu_gain)\n",
    "        self.conv3.weight.data.mul_(relu_gain)\n",
    "        self.linear1.weight.data.mul_(relu_gain)\n",
    "\n",
    "    def forward(self, inputs, states, masks):\n",
    "        x = self.conv1(inputs / 255.0)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.critic_linear(x), x, states\n",
    "    \n",
    "\n",
    "# Necessary for my KFAC implementation.\n",
    "class AddBias(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super(AddBias, self).__init__()\n",
    "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            bias = self._bias.t().view(1, -1)\n",
    "        else:\n",
    "            bias = self._bias.t().view(1, -1, 1, 1)\n",
    "\n",
    "        return x + bias\n",
    "\n",
    "# A temporary solution from the master branch.\n",
    "# https://github.com/pytorch/pytorch/blob/7752fe5d4e50052b3b0bbc9109e599f8157febc0/torch/nn/init.py#L312\n",
    "# Remove after the next version of PyTorch gets release.\n",
    "def orthogonal(tensor, gain=1):\n",
    "    if tensor.ndimension() < 2:\n",
    "        raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n",
    "\n",
    "    rows = tensor.size(0)\n",
    "    cols = tensor[0].numel()\n",
    "    flattened = torch.Tensor(rows, cols).normal_(0, 1)\n",
    "\n",
    "    if rows < cols:\n",
    "        flattened.t_()\n",
    "\n",
    "    # Compute the qr factorization\n",
    "    q, r = torch.qr(flattened)\n",
    "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = torch.diag(r, 0)\n",
    "    ph = d.sign()\n",
    "    q *= ph.expand_as(q)\n",
    "\n",
    "    if rows < cols:\n",
    "        q.t_()\n",
    "\n",
    "    tensor.view_as(q).copy_(q)\n",
    "    tensor.mul_(gain)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS ON: True\n",
      "state shape: 372\n",
      "action shape: 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "from util import flatten_state, flatten_state_no_board\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility, characters\n",
    "from pommerman.constants import Action\n",
    "from collections import deque\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('CUDA IS ON: {}'.format(use_cuda))\n",
    "if use_cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def get_cuda(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "\n",
    "#n_inputs = env.observation_space.shape[0]\n",
    "n_inputs = 372\n",
    "n_hidden = 500\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "print('state shape:', n_inputs)\n",
    "print('action shape:', n_outputs)\n",
    "\n",
    "class TrainingAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber):\n",
    "        super().__init__(character)\n",
    "\n",
    "\n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n",
    "\n",
    "'''\n",
    "Agent class that does nothing\n",
    "'''\n",
    "class StopAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber, *args, **kwargs):\n",
    "        super(StopAgent,self).__init__(character,*args, **kwargs)\n",
    "    \n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.env_name='PongNoFrameskip-v4'\n",
    "        self.seed=1\n",
    "        self.log_dir=''\n",
    "        self.save_dir='saved_models'\n",
    "        self.cuda=False\n",
    "        self.num_stack=4\n",
    "        self.num_steps=5\n",
    "        self.num_processes=1\n",
    "        self.lr=7e-4\n",
    "        self.eps=1e-5\n",
    "        self.alpha=.99\n",
    "        self.max_grad_norm=.5\n",
    "        self.value_loss_coef=.5\n",
    "        self.entropy_coef=.1\n",
    "        self.num_frames=1000\n",
    "        self.use_gae=False\n",
    "        self.gamma=.99\n",
    "        self.tau=.95\n",
    "        self.save_interval=1000\n",
    "        self.log_interval=100\n",
    "        self.vis_interval=100\n",
    "        self.load_model=False\n",
    "        self.save_model=False\n",
    "        \n",
    "args = args()\n",
    "\n",
    "SAVE_PATH = \"saved_models/a2c_121717.pt\"\n",
    "LOAD_PATH = \"saved_models/a2c_121717.pt\"\n",
    "\n",
    "num_updates = 100\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pomme instance>\n",
      "6\n",
      "<generator object Module.parameters at 0x0000021B346CCCA8>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sdadadad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-de6db133cd65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m                        value_loss.data[0], action_loss.data[0]))\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-de6db133cd65>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msdadadad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sdadadad' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    #envs = make_env(args.env_name, args.seed, 0, args.log_dir)\n",
    "\n",
    "    #if args.num_processes > 1:\n",
    "    #    envs = SubprocVecEnv(envs)\n",
    "    #else:\n",
    "    #    envs = DummyVecEnv(envs)\n",
    "\n",
    "    #obs_shape = 372\n",
    "    #obs_shape = (obs_shape[0] * args.num_stack, *obs_shape[1:])\n",
    "    print(env)\n",
    "    print(env.action_space.n)\n",
    "    actor_critic = CNNPolicy(372, env.action_space)\n",
    "  \n",
    "    #if args.load_model:\n",
    "    #    actor_critic.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "    #action_shape = 1\n",
    "\n",
    "    if args.cuda:\n",
    "        actor_critic.cuda()\n",
    "\n",
    "    optimizer = optim.RMSprop(actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space, actor_critic.state_size)\n",
    "    \n",
    "    #current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "    def update_current_obs(obs):\n",
    "        shape_dim0 = envs.observation_space.shape[0]\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        if args.num_stack > 1:\n",
    "            current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "        current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "    obs = envs.reset()\n",
    "    update_current_obs(obs)\n",
    "\n",
    "    rollouts.observations[0].copy_(current_obs)\n",
    "    \n",
    "    episode_rewards = torch.zeros([args.num_processes, 1])\n",
    "    final_rewards = torch.zeros([args.num_processes, 1])\n",
    "\n",
    "    if args.cuda:\n",
    "        current_obs = current_obs.cuda()\n",
    "        rollouts.cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(num_updates):\n",
    "        for step in range(args.num_steps):\n",
    "            \n",
    "            value, action, action_log_prob, states = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                                      Variable(rollouts.states[step], volatile=True),\n",
    "                                                                      Variable(rollouts.masks[step], volatile=True))\n",
    "            cpu_actions = action.data.squeeze(1).cpu().numpy()\n",
    "\n",
    "            obs, reward, done, info = envs.step(cpu_actions)\n",
    "            reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "            episode_rewards += reward\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            final_rewards *= masks\n",
    "            final_rewards += (1 - masks) * episode_rewards\n",
    "            episode_rewards *= masks\n",
    "\n",
    "            if args.cuda:\n",
    "                masks = masks.cuda()\n",
    "\n",
    "            if current_obs.dim() == 4:\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "            else:\n",
    "                current_obs *= masks\n",
    "\n",
    "            update_current_obs(obs)\n",
    "            rollouts.insert(step, current_obs, states.data, action.data, action_log_prob.data, value.data, reward, masks)\n",
    "\n",
    "        next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                                  Variable(rollouts.states[-1], volatile=True),\n",
    "                                  Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "\n",
    "        rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "        values, action_log_probs, dist_entropy, states = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                       Variable(rollouts.states[0].view(-1, actor_critic.state_size)),\n",
    "                                                                                       Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                       Variable(rollouts.actions.view(-1, action_shape)))\n",
    "\n",
    "        values = values.view(args.num_steps, args.num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(args.num_steps, args.num_processes, 1)\n",
    "\n",
    "        advantages = Variable(rollouts.returns[:-1]) - values\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef).backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(actor_critic.parameters(), args.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    " \n",
    "        rollouts.after_update()\n",
    "\n",
    "        if j % args.save_interval == 0 and args.save_model:\n",
    "            save_model = actor_critic\n",
    "            if args.cuda:\n",
    "                save_model = copy.deepcopy(actor_critic).cpu()\n",
    "                \n",
    "            torch.save(actor_critic.state_dict(), SAVE_PATH)\n",
    "\n",
    "        if j % args.log_interval == 0:\n",
    "            end = time.time()\n",
    "            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "            print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "                format(j, total_num_steps,\n",
    "                       int(total_num_steps / (end - start)),\n",
    "                       final_rewards.mean(),\n",
    "                       final_rewards.median(),\n",
    "                       final_rewards.min(),\n",
    "                       final_rewards.max(), dist_entropy.data[0],\n",
    "                       value_loss.data[0], action_loss.data[0]))\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS ON: False\n",
      "state shape: 372\n",
      "action shape: 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "from util import flatten_state, flatten_state_no_board\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility, characters\n",
    "from pommerman.constants import Action\n",
    "from collections import deque\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "use_cuda = False#torch.cuda.is_available()\n",
    "print('CUDA IS ON: {}'.format(use_cuda))\n",
    "#if use_cuda:\n",
    "#    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def get_cuda(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "#n_inputs = env.observation_space.shape[0]\n",
    "n_inputs = 372\n",
    "n_hidden = 500\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "print('state shape:', n_inputs)\n",
    "print('action shape:', n_outputs)\n",
    "\n",
    "class TrainingAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber):\n",
    "        super().__init__(character)\n",
    "\n",
    "\n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n",
    "\n",
    "'''\n",
    "Agent class that does nothing\n",
    "'''\n",
    "class StopAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber, *args, **kwargs):\n",
    "        super(StopAgent,self).__init__(character,*args, **kwargs)\n",
    "    \n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n",
    "    \n",
    "# Add four random agents\n",
    "agents = []\n",
    "#for agent_id in range(4):\n",
    "#    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "agents = {\n",
    "    '0' : SimpleAgent(config[\"agent\"](0, config[\"game_type\"])),\n",
    "    '1' : SimpleAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "    '2' : SimpleAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "    '3' : TrainingAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "}\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_training_agent(3)\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flattens a state s on the form list<dict> where each dict contains information of a state\n",
    "def flatten_state(s):\n",
    "    # Usage Example:\n",
    "\t# def forward(self, x):\n",
    "\t#     x = flatten_state(x)\n",
    "\t# where x is np.atleast1d(S[0])\n",
    "\treturn torch.from_numpy(np.array([flatten_state_aux(x) for x in s])).float()\n",
    "\n",
    "\n",
    "def flatten_state_aux(s):\n",
    "    # Lists\n",
    "    #print (\"---------------------------\")\n",
    "    #print (s)\n",
    "    #print (\"---------------------------\")\n",
    "    alive = [1 if x in s['alive'] else 0 for x in range(10,14)]\n",
    "    board = s['board']\n",
    "    bomb_blast_strength = s['bomb_blast_strength']\n",
    "    bomb_life = s['bomb_life']\n",
    "    # Tuples\n",
    "    position = s['position']\n",
    "    # Ints\n",
    "    blast_strength = s['blast_strength']\n",
    "    can_kick = s['can_kick']\n",
    "    ammo = s['ammo']\n",
    "    # Enums\n",
    "    teammate = s['teammate'] #9 for FFA\n",
    "    enemies = s['enemies'] #11,12,13 for FFA and training agent id = 0\n",
    "\n",
    "    a = np.append(np.array(alive),np.array(board).flatten())\n",
    "    a = np.append(a,np.array(bomb_blast_strength).flatten())\n",
    "    a = np.append(a,np.array(bomb_life).flatten())\n",
    "    a = np.append(a,position[0])\n",
    "    a = np.append(a,position[1])\n",
    "    a = np.append(a,blast_strength)\n",
    "    a = np.append(a,can_kick)\n",
    "    a = np.append(a,ammo)\n",
    "    # Commented out as we get size 376 but expected 372. I assume we calculated wrong.\n",
    "    # Makes sense to ignore these imo\n",
    "    #a = np.append(a,teammate.value)\n",
    "    #a = np.append(a,[e.value for e in enemies])\n",
    "    return a.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def calc_actual_state_values(rewards, dones):\n",
    "    R = []\n",
    "    rewards.reverse()\n",
    "\n",
    "    #print(rewards)\n",
    "    #print(dones)\n",
    "    # If we happen to end the set on a terminal state, set next return to zero\n",
    "    if dones[-1] == True: next_return = 0\n",
    "        \n",
    "    # If not terminal state, bootstrap v(s) using our critic\n",
    "    # TODO: don't need to estimate again, just take from last value of v(s) estimates\n",
    "    else: \n",
    "        print(\"no\")\n",
    "        s = torch.from_numpy(states[-1]).float().unsqueeze(0)\n",
    "        next_return = model.get_state_value(Variable(s)).data[0][0] \n",
    "    \n",
    "    # Backup from last state to calculate \"true\" returns for each state in the set\n",
    "    R.append(next_return)\n",
    "    dones.reverse()\n",
    "    for r in range(1, len(rewards)):\n",
    "        if not dones[r]: this_return = rewards[r] + next_return * GAMMA\n",
    "        else: this_return = 0\n",
    "        R.append(this_return)\n",
    "        next_return = this_return\n",
    "\n",
    "    R.reverse()\n",
    "    state_values_true = Variable(torch.FloatTensor(R)).unsqueeze(1)\n",
    "    \n",
    "    return state_values_true\n",
    "\n",
    "def reflect(states, actions, rewards, dones):\n",
    "    \n",
    "    # Calculating the ground truth \"labels\" as described above\n",
    "    state_values_true = calc_actual_state_values(rewards,dones)\n",
    "    #print(states)\n",
    "    states = np.asarray(states)\n",
    "    action_probs, state_values_est = model.evaluate_actions(states)\n",
    "    action_log_probs = action_probs.log() \n",
    "    \n",
    "    a = Variable(torch.LongTensor(actions).view(-1,1))\n",
    "    chosen_action_log_probs = action_log_probs.gather(1, a)\n",
    "\n",
    "    # This is also the TD error\n",
    "    advantages = state_values_true - state_values_est\n",
    "\n",
    "    entropy = (action_probs * action_log_probs).sum(1).mean()\n",
    "    action_gain = (chosen_action_log_probs * advantages).mean()\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    total_loss = value_loss - action_gain - 0.0001*entropy\n",
    "    print(\"Total Loss:\",total_loss)\n",
    "    print(\"Reward:\",rewards[0])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.linear1 = nn.Linear(N_INPUTS, 64)\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.linear3 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.actor = nn.Linear(64, N_ACTIONS)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "    \n",
    "    # In a PyTorch model, you only have to define the forward pass. PyTorch computes the backwards pass for you!\n",
    "    def forward(self, x):\n",
    "        x = flatten_state(x)\n",
    "        #print(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.relu(x) \n",
    "        return x\n",
    "    \n",
    "    # Only the Actor head\n",
    "    def get_action_probs(self, x):\n",
    "        x = self(x)\n",
    "        action_probs = F.softmax(self.actor(x),dim=1)\n",
    "        return action_probs\n",
    "    \n",
    "    # Only the Critic head\n",
    "    def get_state_value(self, x):\n",
    "        x = self(x)\n",
    "        state_value = self.critic(x)\n",
    "        return state_value\n",
    "    \n",
    "    # Both heads\n",
    "    def evaluate_actions(self, x):\n",
    "        #print(\"y\")\n",
    "        x = self(x)\n",
    "        #print(\"no\")\n",
    "        #print(x)\n",
    "        action_probs = F.softmax(self.actor(x),dim=1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_probs, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pomme instance>\n",
      "Game: 0\n",
      "Total Loss: tensor(0.0179, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2065, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5127, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6577, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7552, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6603, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7193, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5798, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8145, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8524, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6953, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6884, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8831, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6980, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7590, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6559, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6659, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7558, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7846, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7242, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6792, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6362, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6843, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4755, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2883, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.0449, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6915, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7452, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7033, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6403, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6881, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6647, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6586, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7157, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.9354, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6364, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6314, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7800, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6604, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4796, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8363, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7091, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8076, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.9686, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4953, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4020, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4478, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4378, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7299, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5531, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3926, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3296, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4912, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6513, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4135, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5420, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4866, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2255, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(0.1166, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(0.4130, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.0332, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1346, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1593, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2142, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1898, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1839, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1137, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1352, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1373, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(0.0020, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1724, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2357, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1864, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1801, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(0.0156, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.0543, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1756, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1288, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3009, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3535, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2670, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1322, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2470, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2378, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1229, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1705, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.0656, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1983, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1978, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1913, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1258, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1644, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1036, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2527, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3844, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3562, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2662, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2392, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2013, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3008, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Game: 100\n",
      "Total Loss: tensor(-0.3110, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2163, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3174, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3660, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1670, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3235, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3942, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4214, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5120, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4174, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3204, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4772, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4323, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3722, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2721, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2786, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3779, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4437, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4070, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4246, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4675, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3755, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4534, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7369, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6711, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5093, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5874, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4126, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: tensor(-0.4892, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4313, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6242, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5971, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5594, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5618, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5469, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5848, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6030, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3790, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5728, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6771, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5757, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4147, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5001, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4851, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5457, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5817, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5475, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5736, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5835, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5225, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5410, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6890, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5704, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6384, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7919, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5588, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6088, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6295, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6152, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5713, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.0721, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6158, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6056, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5143, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6216, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6245, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6604, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6231, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6072, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6111, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6723, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5712, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6138, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5936, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6008, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6573, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6172, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5745, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5945, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6626, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6018, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6459, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5874, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6246, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6609, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5998, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6363, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7292, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6328, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6320, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6061, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5907, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5701, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6133, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4707, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6293, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5618, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5630, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6190, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6458, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Game: 200\n",
      "Total Loss: tensor(-0.5096, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7115, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6685, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5900, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6064, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7010, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6034, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6960, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8777, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5670, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6511, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6219, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5975, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7130, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5546, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4866, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4471, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8540, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5530, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6058, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5051, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4758, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4377, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4901, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5531, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5554, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5202, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.0924, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5876, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6286, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8436, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7787, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7464, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8356, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5492, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2595, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8716, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1655, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2971, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2789, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4415, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4900, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4437, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4214, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4711, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4298, grad_fn=<ThSubBackward>)\n",
      "Reward: 1\n",
      "Total Loss: tensor(-0.4437, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1469, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2440, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.0601, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4152, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3023, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5247, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3533, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4070, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6464, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: tensor(-0.5215, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5408, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.5260, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6530, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8279, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6059, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6312, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.2781, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.4548, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2541, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2380, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.6250, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.6779, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2156, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3138, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2536, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.9080, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2869, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1892, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4504, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.0200, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.1385, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.5459, grad_fn=<ThSubBackward>)\n",
      "Reward: 1\n",
      "Total Loss: tensor(-0.8156, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-2.5619, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.9136, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-3.1069, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-2.6045, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.4325, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.5513, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.3363, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-4.1214, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-3.4176, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.1010, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(1.2885, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-9.6087, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(5.8647, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-1.8297, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.8468, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.2008, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-2.6901, grad_fn=<ThSubBackward>)\n",
      "Reward: 1\n",
      "Total Loss: tensor(-2.3469, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-3.1849, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-2.9390, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Game: 300\n",
      "Total Loss: tensor(-1.7776, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-16.2191, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(0.4264, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-14.2247, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-2.7029, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-16.7140, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-12.2478, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-7.5955, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-3.2626, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-3.1827, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(2.2346, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-22.4789, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-0.7411, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-49.4254, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-4.3320, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-4.5898, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-12.3103, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(-6.2052, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Total Loss: tensor(nan, grad_fn=<ThSubBackward>)\n",
      "Reward: -1\n",
      "Error, action probs: tensor([[nan, nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-b274b326f72f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Reflect on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mreflect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-131-b04c5e826c0d>\u001b[0m in \u001b[0;36mreflect\u001b[1;34m(states, actions, rewards, dones)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Calculating the ground truth \"labels\" as described above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mstate_values_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_actual_state_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;31m#print(states)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-131-b04c5e826c0d>\u001b[0m in \u001b[0;36mcalc_actual_state_values\u001b[1;34m(rewards, dones)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#print(dones)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# If we happen to end the set on a terminal state, set next return to zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnext_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# If not terminal state, bootstrap v(s) using our critic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Discount factor. Model is not very sensitive to this value.\n",
    "GAMMA = .95\n",
    "\n",
    "# LR of 3e-2 explodes the gradients, LR of 3e-4 trains slower\n",
    "LR = 3e-3\n",
    "N_GAMES = 1000\n",
    "\n",
    "# OpenAI baselines uses nstep of 5.\n",
    "#N_STEPS = 20\n",
    "\n",
    "N_ACTIONS = 6 # get from env\n",
    "N_INPUTS = 372 # get from env\n",
    "\n",
    "model = ActorCritic()\n",
    "\n",
    "if use_cuda:\n",
    "    print(\"wat\")\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "state = env.reset()\n",
    "finished_games = 0\n",
    "\n",
    "print(env)\n",
    "action_probs = []\n",
    "for i in range(N_GAMES):\n",
    "    states, actions, rewards, dones = [], [], [], []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    if(i%100 == 0):\n",
    "        print(\"Game:\",i)\n",
    "    # Gather training data\n",
    "    try:\n",
    "        while not done:\n",
    "            \n",
    "            s = np.atleast_1d(state[3])\n",
    "            #print(s)\n",
    "\n",
    "            action_probs = model.get_action_probs(s)\n",
    "            a = action_probs.multinomial(1)[0].item()\n",
    "\n",
    "            acts = env.act(state)\n",
    "            acts.append(a)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(acts)\n",
    "\n",
    "            states.append(state[3]); actions.append(a); rewards.append(reward[3]); dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "    except RuntimeError:\n",
    "        print(\"Error, action probs:\",action_probs)\n",
    "\n",
    "    # Reflect on training data\n",
    "    reflect(states, actions, rewards, dones)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
