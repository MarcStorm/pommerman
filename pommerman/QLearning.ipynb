{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility, characters\n",
    "from pommerman.constants import Action\n",
    "from collections import deque\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_cuda(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "# Flattens a state s on the form list<dict> where each dict contains information of a state\n",
    "def flatten_state(s):\n",
    "    # Usage Example:\n",
    "\t# def forward(self, x):\n",
    "\t#     x = flatten_state(x)\n",
    "\t# where x is np.atleast1d(S[0])\n",
    "\treturn torch.from_numpy(np.array([flatten_state_aux(x) for x in s])).float()\n",
    "\t\n",
    "\n",
    "def flatten_state_aux(s):\n",
    "    # Lists\n",
    "    #print (\"---------------------------\")\n",
    "    #print (s)\n",
    "    #print (\"---------------------------\")\n",
    "    alive = [1 if x in s['alive'] else 0 for x in range(10,14)]\n",
    "    board = s['board']\n",
    "    bomb_blast_strength = s['bomb_blast_strength']\n",
    "    bomb_life = s['bomb_life']\n",
    "    # Tuples\n",
    "    position = s['position']\n",
    "    # Ints\n",
    "    blast_strength = s['blast_strength']\n",
    "    can_kick = s['can_kick']\n",
    "    ammo = s['ammo']\n",
    "    # Enums\n",
    "    teammate = s['teammate'] #9 for FFA\n",
    "    enemies = s['enemies'] #11,12,13 for FFA and training agent id = 0\n",
    "    \n",
    "    a = np.append(np.array(alive),np.array(board).flatten())\n",
    "    a = np.append(a,np.array(bomb_blast_strength).flatten())\n",
    "    a = np.append(a,np.array(bomb_life).flatten())\n",
    "    a = np.append(a,position[0])\n",
    "    a = np.append(a,position[1])\n",
    "    a = np.append(a,blast_strength)\n",
    "    a = np.append(a,can_kick)\n",
    "    a = np.append(a,ammo)\n",
    "    # Commented out as we get size 376 but expected 372. I assume we calculated wrong.\n",
    "    # Makes sense to ignore these imo\n",
    "    #a = np.append(a,teammate.value)\n",
    "    #a = np.append(a,[e.value for e in enemies])\n",
    "    return a.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions: \n",
    "    Stop = 0,\n",
    "    Up = 1,\n",
    "    Down = 2,\n",
    "    Left = 3,\n",
    "    Right = 4,\n",
    "    Bomb = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        x = flatten_state(x)\n",
    "        x = get_cuda(x)\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
    "    \n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoder for the states\n",
    "def one_hot(i, l):\n",
    "    a = np.zeros((len(i), l))\n",
    "    a[range(len(i)), i] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: 372\n",
      "action shape: 6\n"
     ]
    }
   ],
   "source": [
    "#n_inputs = env.observation_space.shape[0]\n",
    "n_inputs = 372\n",
    "#n_hidden = 20\n",
    "n_hidden = 500\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "print('state shape:', n_inputs)\n",
    "print('action shape:', n_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingAgent(BaseAgent):\n",
    "    \n",
    "    def __init__(self, character=characters.Bomber):\n",
    "        super().__init__(character)\n",
    "        \n",
    "        \n",
    "    def act(self, obs, action_space):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefill replay memory\n",
      "start training\n"
     ]
    }
   ],
   "source": [
    "# train Deep Q-network\n",
    "\n",
    "num_episodes = 1000\n",
    "#episode_limit = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "gamma = 0.99 # discount rate\n",
    "tau = 0.01 # target network update rate\n",
    "replay_memory_capacity = 10000\n",
    "prefill_memory = True\n",
    "val_freq = 100 # validation frequency\n",
    "\n",
    "# initialize DQN and replay memory\n",
    "policy_dqn = DQN(n_inputs, n_outputs, learning_rate)\n",
    "target_dqn = DQN(n_inputs, n_outputs, learning_rate)\n",
    "target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "if use_cuda:\n",
    "    policy_dqn.cuda()\n",
    "    target_dqn.cuda()\n",
    "\n",
    "replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "\n",
    "# Add four random agents\n",
    "agents = []\n",
    "#for agent_id in range(4):\n",
    "#    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "agents = {\n",
    "    '0' : SimpleAgent(config[\"agent\"](0, config[\"game_type\"])),\n",
    "    '1' : RandomAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "    '2' : RandomAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "    '3' : TrainingAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "}\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_training_agent(3)\n",
    "env.set_init_game_state(None)   \n",
    "\n",
    "# prefill replay memory with random actions\n",
    "if prefill_memory:\n",
    "    print('prefill replay memory')\n",
    "    \n",
    "    s = env.reset()\n",
    "    while replay_memory.count() < replay_memory_capacity:\n",
    "        a = env.act(s)\n",
    "        a.append(0)\n",
    "        s1, r, d, _ = env.step(a)\n",
    "        replay_memory.add(s[3], a[3], r[3], s1[3], d)\n",
    "        s = s1 if not d else env.reset()\n",
    "        \n",
    "# training loop\n",
    "try:\n",
    "    print('start training')\n",
    "    epsilon = 1.0\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        # Add four random agents\n",
    "        #agents = []\n",
    "        #for agent_id in range(4):\n",
    "        #    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "        #agents = {\n",
    "        #    '0' : SimpleAgent(config[\"agent\"](0, config[\"game_type\"])),\n",
    "        #    '1' : RandomAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "        #    '2' : RandomAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "        #   '3' : TrainingAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "        #}\n",
    "        #env.set_agents(list(agents.values()))\n",
    "        #env.set_training_agent(3)\n",
    "        #env.set_init_game_state(None)\n",
    "        s = env.reset()\n",
    "        \n",
    "        # init new episode\n",
    "        ep_reward, ep_loss = 0, 0\n",
    "        d = False\n",
    "        j = -1\n",
    "        while not d:\n",
    "            j += 1\n",
    "            # select action with epsilon-greedy strategy\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():         \n",
    "                    a = get_numpy(policy_dqn(np.atleast_1d(s[3]))).argmax().item()\n",
    "            # perform action\n",
    "            actions = env.act(s)\n",
    "            actions.append(a)\n",
    "            s1, r, d, _ = env.step(actions)\n",
    "            # store experience in replay memory\n",
    "            replay_memory.add(s[3], a, r[3], s1[3], d)\n",
    "            # batch update\n",
    "            if replay_memory.count() >= batch_size:\n",
    "                # sample batch from replay memory\n",
    "                batch = np.array(replay_memory.sample(batch_size))\n",
    "                ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]\n",
    "                # do forward pass of batch\n",
    "                policy_dqn.optimizer.zero_grad()\n",
    "                \n",
    "                Q = policy_dqn(ss)\n",
    "                # use target network to compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    # TODO: use target net\n",
    "                    Q1 = target_dqn(ss1)\n",
    "                # compute target for each sampled experience\n",
    "                q_targets = Q.clone()\n",
    "                for k in range(batch_size):\n",
    "                    q_targets[k, aa[k]] = rr[k] + gamma * Q1[k].max().item() * (not dd[k])\n",
    "                # update network weights\n",
    "                loss = policy_dqn.loss(Q, q_targets)\n",
    "                loss.backward()\n",
    "                policy_dqn.optimizer.step()\n",
    "                # update target network parameters from policy network parameters\n",
    "                target_dqn.update_params(policy_dqn.state_dict(), tau)\n",
    "            else:\n",
    "                loss = 0\n",
    "            # bookkeeping\n",
    "            s = s1\n",
    "            ep_reward += r[3]\n",
    "            ep_loss += loss.item()\n",
    "        # bookkeeping\n",
    "        #epsilon = epsilon\n",
    "        epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "        if (i+1) % val_freq == 0: print('%5d mean training reward: %5.2f' % (i+1, np.mean(rewards[-val_freq:])))\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch = np.array((replay_memory.sample(batch_size)))\n",
    "#ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]\n",
    "#print (ss[0])\n",
    "#[1 if x in ss[0]['alive'] else 0 for x in range(10,14)]\n",
    "#print (s[3])\n",
    "#flatten_state(s[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(411)\n",
    "plt.title('training rewards')\n",
    "plt.plot(range(1, num_episodes+1), rewards)\n",
    "plt.plot(moving_average(rewards))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(412)\n",
    "plt.title('training lengths')\n",
    "plt.plot(range(1, num_episodes+1), lengths)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(lengths))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(413)\n",
    "plt.title('training loss')\n",
    "plt.plot(range(1, num_episodes+1), losses)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(losses))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(414)\n",
    "plt.title('epsilon')\n",
    "plt.plot(range(1, num_episodes+1), epsilons)\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save file\n",
    "PATH = \"resources/qAgent.pt\"\n",
    "torch.save(policy_dqn.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # you know this by now\n",
    "n_inputs = 372\n",
    "n_hidden = 500\n",
    "n_outputs = 6\n",
    "\n",
    "class FirstAgent(BaseAgent):\n",
    "    \n",
    "    def __init__(self, character=characters.Bomber):\n",
    "        super().__init__(character)\n",
    "        self.policy_dqn = DQN(n_inputs, n_outputs, learning_rate)\n",
    "        state_list = torch.load(PATH)\n",
    "        self.policy_dqn.load_state_dict(state_list)\n",
    "        \n",
    "        \n",
    "    def act(self, obs, action_space):\n",
    "        # Kald neuralt netvÃ¦rk og return\n",
    "        with torch.no_grad():\n",
    "            a_prob = self.policy_dqn(np.atleast_1d(obs))\n",
    "        a = (np.cumsum(a_prob.numpy()) > np.random.rand()).argmax() # sample action\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add four random agents\n",
    "agents = []\n",
    "#for agent_id in range(4):\n",
    "#    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "agents = {\n",
    "    '0' : SimpleAgent(config[\"agent\"](0, config[\"game_type\"])),\n",
    "    '1' : RandomAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "    '2' : RandomAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "    '3' : FirstAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "}\n",
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seed and reset the environment\n",
    "env.seed(0)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the random agents until we're done\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(obs)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "\n",
    "print(info)\n",
    "print (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating DataFrame from targets_data\n",
    "#targets_df = pd.DataFrame(data=targets_data)\n",
    "#targets_df.columns = ['targets']\n",
    "\n",
    "# creating tensor from targets_df \n",
    "#torch_tensor = torch.tensor(targets_df['targets'].values)\n",
    "\n",
    "# printing out result\n",
    "#print(torch_tensor\n",
    "dataFrame = pd.DataFrame(data=s)\n",
    "columns = dataFrame.columns \n",
    "torchTenssor = dataFrame[columns].values\n",
    "\n",
    "#print(dataFrame.values.ravel('C'))\n",
    "#print(dataFrame.stack())\n",
    "#print (torch.tensor(.columns.values))\n",
    "#print (obs[3]['board'])\n",
    "#print (obs[3]['bomb_blast_strength'])\n",
    "#print (obs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
