{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "from util import flatten_state, flatten_state_no_board\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility, characters\n",
    "from pommerman.constants import Action\n",
    "from collections import deque\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "# Add four random agents\n",
    "agents = []\n",
    "#for agent_id in range(4):\n",
    "#    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "agents = {\n",
    "    '0' : RandomAgent(config[\"agent\"](0, config[\"game_type\"])),\n",
    "    '1' : RandomAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "    '2' : SimpleAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "    '3' : TrainingAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "}\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_training_agent(3)\n",
    "env.set_init_game_state(None)\n",
    "\n",
    "# prefill replay memory with random actions\n",
    "if prefill_memory:\n",
    "    print('prefill replay memory')\n",
    "\n",
    "    s = env.reset()\n",
    "    while replay_memory.count() < replay_memory_capacity:\n",
    "        a = env.act(s)\n",
    "        a.append(0)\n",
    "        s1, r, d, _ = env.step(a)\n",
    "        replay_memory.add(s[3], a[3], r[3], s1[3], d)\n",
    "        s = s1 if not d else env.reset()\n",
    "\n",
    "# training loop\n",
    "try:\n",
    "    print('start training')\n",
    "    epsilon = 1.0\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "\n",
    "        # init new episode\n",
    "        ep_reward, ep_loss = 0, 0\n",
    "        d = False\n",
    "        j = -1\n",
    "        while not d:\n",
    "            j += 1\n",
    "            # select action with epsilon-greedy strategy\n",
    "#            if np.random.rand() < epsilon:\n",
    "            #a = env.action_space.sample()\n",
    "#            else:\n",
    "#                with torch.no_grad():\n",
    "#                    a = get_numpy(policy_dqn(np.atleast_1d(s[3]))).argmax().item()\n",
    "            # perform action\n",
    "            #actions = env.act(s)\n",
    "            actions.append(a)\n",
    "            s1, r, d, _ = env.step(actions)\n",
    "\n",
    "        #epsilon = epsilon\n",
    "        epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "        if (i+1) % val_freq == 0:\n",
    "            print('%5d mean training reward: %5.2f' % (i+1, np.mean(rewards[-val_freq:])))\n",
    "            # Save file\n",
    "            t = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "            PATH = \"resources/q_agent_{}.pt\".format(t)\n",
    "            torch.save(policy_dqn.state_dict(), PATH)\n",
    "            print(\"File saved\")\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS ON: False\n",
      "state shape: 372\n",
      "action shape: 6\n",
      "prefill replay memory\n",
      "start training\n",
      "Steps : 500 Time : 79.81902265548706\n",
      "Steps : 1000 Time : 165.69096684455872\n",
      "Steps : 1500 Time : 252.15001702308655\n",
      "Steps : 2000 Time : 338.8155822753906\n",
      "Steps : 2500 Time : 424.6773030757904\n",
      "Steps : 3000 Time : 510.1430780887604\n",
      "interrupt\n",
      "Function took -1542639813.6153493 sec\n",
      "\n",
      "Total steps: 3028\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "from util import flatten_state, flatten_state_no_board\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility, characters\n",
    "from pommerman.constants import Action\n",
    "from collections import deque\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "use_cuda = False\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "print('CUDA IS ON: {}'.format(use_cuda))\n",
    "if use_cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def get_cuda(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "batch_norm=False\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "kernel_size = 5\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.other_shape = [3]\n",
    "\n",
    "        #Input for conv2d is (batch_size, num_channels, width, height)\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels=out_channels,\n",
    "                               kernel_size=kernel_size, stride=1, padding=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = in_channels, out_channels=out_channels,\n",
    "                               kernel_size=kernel_size, stride=1, padding=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = in_channels, out_channels=out_channels,\n",
    "                               kernel_size=kernel_size, stride=1, padding=2)\n",
    "\n",
    "        self.convolution_out_size = 11*11*3\n",
    "\n",
    "        self.ffn_input_size = n_inputs\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(n_hidden, n_outputs),\n",
    "        )\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(11)\n",
    "        else:\n",
    "            self.bn1 = lambda x: x\n",
    "            self.bn2 = lambda x: x\n",
    "            self.bn3 = lambda x: x\n",
    "\n",
    "        self.ffn.apply(self.init_weights)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        board_length = len(x[0]['board'])\n",
    "        completeBoard = [[\n",
    "                        [[state['board'][x,y] for y in range(board_length)] for x in range(board_length)],\n",
    "                        [[state['bomb_blast_strength'][x,y] for y in range(board_length)] for x in range(board_length)],\n",
    "                        [[state['bomb_life'][x,y] for y in range(board_length)] for x in range(board_length)]\n",
    "                        ] for state in x]\n",
    "\n",
    "        completeBoard = np.asarray(completeBoard)\n",
    "        completeBoard = torch.tensor(completeBoard)\n",
    "        completeBoard = completeBoard.float()\n",
    "        boardVariable = torch.autograd.Variable(completeBoard)\n",
    "        board = self.conv1(boardVariable)\n",
    "        board = self.bn1(board)\n",
    "        board = self.activation(board)\n",
    "        board = self.conv2(board)\n",
    "        board = self.bn1(board)\n",
    "        board = self.activation(board)\n",
    "        board = self.conv3(board)\n",
    "        board = self.bn1(board)\n",
    "        board = self.activation(board)\n",
    "        x2 = board.view(-1, self.convolution_out_size)\n",
    "        x = flatten_state_no_board(x)\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            x2 = x2.cuda()\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.ffn(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x \n",
    "\n",
    "    def loss(self, action_probabilities, returns):\n",
    "        return -torch.mean(torch.mul(torch.log(action_probabilities), returns))\n",
    "\n",
    "    def init_weights(m, *args):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    # Function from old network\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)\n",
    "\n",
    "    # The commented code below is the old network.\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    '''\n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = flatten_state(x)\n",
    "        x = get_cuda(x)\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
    "\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)\n",
    "    '''\n",
    "\n",
    "# one-hot encoder for the states\n",
    "def one_hot(i, l):\n",
    "    a = np.zeros((len(i), l))\n",
    "    a[range(len(i)), i] = 1\n",
    "    return a\n",
    "\n",
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "#n_inputs = env.observation_space.shape[0]\n",
    "n_inputs = 372\n",
    "n_hidden = 500\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "print('state shape:', n_inputs)\n",
    "print('action shape:', n_outputs)\n",
    "\n",
    "class TrainingAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber):\n",
    "        super().__init__(character)\n",
    "\n",
    "\n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n",
    "\n",
    "'''\n",
    "Agent class that does nothing\n",
    "'''\n",
    "class StopAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber, *args, **kwargs):\n",
    "        super(StopAgent,self).__init__(character,*args, **kwargs)\n",
    "    \n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n",
    "\n",
    "# train Deep Q-network\n",
    "\n",
    "num_episodes = 1000000\n",
    "#episode_limit = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "gamma = 1 # discount rate\n",
    "tau = 0.01 # target network update rate\n",
    "replay_memory_capacity = 10000\n",
    "prefill_memory = True\n",
    "val_freq = 100000 # validation frequency\n",
    "\n",
    "# initialize DQN and replay memory\n",
    "policy_dqn = DQN(n_inputs, n_hidden, n_outputs, learning_rate)\n",
    "target_dqn = DQN(n_inputs, n_hidden, n_outputs, learning_rate)\n",
    "target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "if use_cuda:\n",
    "    policy_dqn.cuda()\n",
    "    target_dqn.cuda()\n",
    "\n",
    "replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "\n",
    "# Add four random agents\n",
    "agents = []\n",
    "#for agent_id in range(4):\n",
    "#    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "agents = {\n",
    "    '0' : RandomAgent(config[\"agent\"](0, config[\"game_type\"])),\n",
    "    '1' : RandomAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "    '2' : RandomAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "    '3' : TrainingAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "    #'2' : SimpleAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "    #'3' : TrainingAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "}\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_training_agent(3)\n",
    "env.set_init_game_state(None)\n",
    "\n",
    "# prefill replay memory with random actions\n",
    "if prefill_memory:\n",
    "    print('prefill replay memory')\n",
    "\n",
    "    s = env.reset()\n",
    "    while replay_memory.count() < replay_memory_capacity:\n",
    "        a = env.act(s)\n",
    "        a.append(0)\n",
    "        s1, r, d, _ = env.step(a)\n",
    "        replay_memory.add(s[3], a[3], r[3], s1[3], d)\n",
    "        s = s1 if not d else env.reset()\n",
    "\n",
    "# training loop\n",
    "try:\n",
    "    print('start training')\n",
    "    epsilon = 0\n",
    "    totalSteps = 0\n",
    "    time1 = time.time()\n",
    "    time2 = 0\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "    for i in range(num_episodes):\n",
    "        #if totalSteps%1000 == 0:\n",
    "        #    print(\"Steps :\",totalSteps)\n",
    "        s = env.reset()\n",
    "\n",
    "        # init new episode\n",
    "        ep_reward, ep_loss = 0, 0\n",
    "        d = False\n",
    "        steps = 0\n",
    "        while not d:\n",
    "            totalSteps += 1\n",
    "            if totalSteps%500 == 0:\n",
    "                print(\"Steps :\",totalSteps, \"Time :\",time.time() - time1)\n",
    "            # select action with epsilon-greedy strategy\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a = get_numpy(policy_dqn(np.atleast_1d(s[3]))).argmax().item()\n",
    "            # perform action\n",
    "            actions = env.act(s)\n",
    "            actions.append(a)\n",
    "            s1, r, d, _ = env.step(actions)\n",
    "            # store experience in replay memory\n",
    "            replay_memory.add(s[3], a, r[3], s1[3], d)\n",
    "            # batch update\n",
    "            if replay_memory.count() >= batch_size:\n",
    "                # sample batch from replay memory\n",
    "                batch = np.array(replay_memory.sample(batch_size))\n",
    "                ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]\n",
    "                # do forward pass of batch\n",
    "                policy_dqn.optimizer.zero_grad()\n",
    "                Q = policy_dqn(ss)\n",
    "                #print(\"\\nQ\",Q)\n",
    "                # use target network to compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    # TODO: use target net\n",
    "                    Q1 = target_dqn(ss1)\n",
    "                #print(\"\\nQ1\",Q1)\n",
    "                # compute target for each sampled experience\n",
    "                q_targets = Q.clone()\n",
    "                for k in range(batch_size):\n",
    "                    q_targets[k, aa[k]] = rr[k] + gamma * Q1[k].max().item() * (not dd[k])\n",
    "                \n",
    "                # update network weights\n",
    "                #print(\"\\nq_targets\",q_targets)\n",
    "                loss = policy_dqn.loss(Q, q_targets)\n",
    "                #print(\"Before backward:\",loss)\n",
    "                loss.backward()\n",
    "                #print(\"After Backward\",loss,\"\\n\")\n",
    "                policy_dqn.optimizer.step()\n",
    "                # update target network parameters from policy network parameters\n",
    "                target_dqn.update_params(policy_dqn.state_dict(), tau)\n",
    "            else:\n",
    "                loss = 0\n",
    "            # bookkeeping\n",
    "            s = s1\n",
    "            ep_reward += r[3]\n",
    "            ep_loss += loss.item()\n",
    "        # bookkeeping\n",
    "        \n",
    "        #totalSteps += steps\n",
    "        if totalSteps > 20000:\n",
    "            time2 = time.time()\n",
    "            break\n",
    "        \n",
    "        #epsilon = 0#epsilon\n",
    "        #epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "        #epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "        if False: #(i+1) % val_freq == 0:\n",
    "            print('%5d mean training reward: %5.2f' % (i+1, np.mean(rewards[-val_freq:])))\n",
    "            # Save file\n",
    "            t = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "            PATH = \"resources/q_agent_{}.pt\".format(t)\n",
    "            torch.save(policy_dqn.state_dict(), PATH)\n",
    "            print(\"File saved\")\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')\n",
    "    \n",
    "print('Function took',(time2-time1), 'sec')\n",
    "print(\"\\nTotal steps:\", totalSteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
