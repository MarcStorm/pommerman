{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS ON: True\n",
      "state shape: 372\n",
      "action shape: 6\n",
      "prefill replay memory\n",
      "start training\n",
      "    9 mean training reward: -1.00\n",
      "File saved\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(16, 9))\\nplt.subplot(411)\\nplt.title('training rewards')\\nplt.plot(range(1, num_episodes+1), rewards)\\nplt.plot(moving_average(rewards))\\nplt.xlim([0, num_episodes])\\nplt.subplot(412)\\nplt.title('training lengths')\\nplt.plot(range(1, num_episodes+1), lengths)\\nplt.plot(range(1, num_episodes+1), moving_average(lengths))\\nplt.xlim([0, num_episodes])\\nplt.subplot(413)\\nplt.title('training loss')\\nplt.plot(range(1, num_episodes+1), losses)\\nplt.plot(range(1, num_episodes+1), moving_average(losses))\\nplt.xlim([0, num_episodes])\\nplt.subplot(414)\\nplt.title('epsilon')\\nplt.plot(range(1, num_episodes+1), epsilons)\\nplt.xlim([0, num_episodes])\\nplt.tight_layout(); plt.show()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "from util import flatten_state, flatten_state_no_board\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility, characters\n",
    "from pommerman.constants import Action\n",
    "from collections import deque\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('CUDA IS ON: {}'.format(use_cuda))\n",
    "if use_cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def get_cuda(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "# Flattens a state s on the form list<dict> where each dict contains information of a state\n",
    "def flatten_state(s):\n",
    "    # Usage Example:\n",
    "\t# def forward(self, x):\n",
    "\t#     x = flatten_state(x)\n",
    "\t# where x is np.atleast1d(S[0])\n",
    "\treturn torch.from_numpy(np.array([flatten_state_aux(x) for x in s])).float()\n",
    "\n",
    "\n",
    "def flatten_state_aux(s):\n",
    "    # Lists\n",
    "    #print (\"---------------------------\")\n",
    "    #print (s)\n",
    "    #print (\"---------------------------\")\n",
    "    alive = [1 if x in s['alive'] else 0 for x in range(10,14)]\n",
    "    board = s['board']\n",
    "    bomb_blast_strength = s['bomb_blast_strength']\n",
    "    bomb_life = s['bomb_life']\n",
    "    # Tuples\n",
    "    position = s['position']\n",
    "    # Ints\n",
    "    blast_strength = s['blast_strength']\n",
    "    can_kick = s['can_kick']\n",
    "    ammo = s['ammo']\n",
    "    # Enums\n",
    "    teammate = s['teammate'] #9 for FFA\n",
    "    enemies = s['enemies'] #11,12,13 for FFA and training agent id = 0\n",
    "\n",
    "    a = np.append(np.array(alive),np.array(board).flatten())\n",
    "    a = np.append(a,np.array(bomb_blast_strength).flatten())\n",
    "    a = np.append(a,np.array(bomb_life).flatten())\n",
    "    a = np.append(a,position[0])\n",
    "    a = np.append(a,position[1])\n",
    "    a = np.append(a,blast_strength)\n",
    "    a = np.append(a,can_kick)\n",
    "    a = np.append(a,ammo)\n",
    "    # Commented out as we get size 376 but expected 372. I assume we calculated wrong.\n",
    "    # Makes sense to ignore these imo\n",
    "    #a = np.append(a,teammate.value)\n",
    "    #a = np.append(a,[e.value for e in enemies])\n",
    "    return a.astype(float)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "batch_norm=False\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "kernel_size = 5\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.other_shape = [3]\n",
    "\n",
    "        #Input for conv2d is (batch_size, num_channels, width, height)\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels=out_channels,\n",
    "                               kernel_size=kernel_size, stride=1, padding=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = in_channels, out_channels=out_channels,\n",
    "                               kernel_size=kernel_size, stride=1, padding=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = in_channels, out_channels=out_channels,\n",
    "                               kernel_size=kernel_size, stride=1, padding=2)\n",
    "\n",
    "        self.convolution_out_size = 11*11*3\n",
    "\n",
    "        self.ffn_input_size = n_inputs\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(n_hidden, n_outputs),\n",
    "        )\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(11)\n",
    "        else:\n",
    "            self.bn1 = lambda x: x\n",
    "            self.bn2 = lambda x: x\n",
    "            self.bn3 = lambda x: x\n",
    "\n",
    "        self.ffn.apply(self.init_weights)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        board_length = len(x[0]['board'])\n",
    "        completeBoard = [[\n",
    "                        [[state['board'][x,y] for y in range(board_length)] for x in range(board_length)],\n",
    "                        [[state['bomb_blast_strength'][x,y] for y in range(board_length)] for x in range(board_length)],\n",
    "                        [[state['bomb_life'][x,y] for y in range(board_length)] for x in range(board_length)]\n",
    "                        ] for state in x]\n",
    "\n",
    "        completeBoard = np.asarray(completeBoard)\n",
    "        completeBoard = torch.tensor(completeBoard)\n",
    "        completeBoard = completeBoard.float()\n",
    "        boardVariable = torch.autograd.Variable(completeBoard)\n",
    "        board = self.conv1(boardVariable)\n",
    "        board = self.bn1(board)\n",
    "        board = self.activation(board)\n",
    "        board = self.conv2(board)\n",
    "        board = self.bn1(board)\n",
    "        board = self.activation(board)\n",
    "        board = self.conv3(board)\n",
    "        board = self.bn1(board)\n",
    "        board = self.activation(board)\n",
    "        x2 = board.view(-1, self.convolution_out_size)\n",
    "        x = flatten_state_no_board(x)\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            x2 = x2.cuda()\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.ffn(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, action_probabilities, returns):\n",
    "        return -torch.mean(torch.mul(torch.log(action_probabilities), returns))\n",
    "\n",
    "    def init_weights(m, *args):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    # Function from old network\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)\n",
    "\n",
    "    # The commented code below is the old network.\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    '''\n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = flatten_state(x)\n",
    "        x = get_cuda(x)\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
    "\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)\n",
    "    '''\n",
    "\n",
    "# one-hot encoder for the states\n",
    "def one_hot(i, l):\n",
    "    a = np.zeros((len(i), l))\n",
    "    a[range(len(i)), i] = 1\n",
    "    return a\n",
    "\n",
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "\n",
    "#n_inputs = env.observation_space.shape[0]\n",
    "n_inputs = 372\n",
    "n_hidden = 500\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "print('state shape:', n_inputs)\n",
    "print('action shape:', n_outputs)\n",
    "\n",
    "class TrainingAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber):\n",
    "        super().__init__(character)\n",
    "\n",
    "\n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n",
    "\n",
    "'''\n",
    "Agent class that does nothing\n",
    "'''\n",
    "class StopAgent(BaseAgent):\n",
    "\n",
    "    def __init__(self, character=characters.Bomber, *args, **kwargs):\n",
    "        super(StopAgent,self).__init__(character,*args, **kwargs)\n",
    "    \n",
    "    def act(self, obs, action_space):\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# train Deep Q-network\n",
    "\n",
    "num_episodes = 10\n",
    "#episode_limit = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "gamma = 1 # discount rate\n",
    "tau = 0.01 # target network update rate\n",
    "replay_memory_capacity = 100\n",
    "prefill_memory = True\n",
    "val_freq = 9 # validation frequency\n",
    "\n",
    "# initialize DQN and replay memory\n",
    "policy_dqn = DQN(n_inputs, n_hidden, n_outputs, learning_rate)\n",
    "target_dqn = DQN(n_inputs, n_hidden, n_outputs, learning_rate)\n",
    "target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "if use_cuda:\n",
    "    policy_dqn.cuda()\n",
    "    target_dqn.cuda()\n",
    "\n",
    "replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "\n",
    "# Add four random agents\n",
    "agents = []\n",
    "#for agent_id in range(4):\n",
    "#    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "agents = {\n",
    "    '0' : SimpleAgent(config[\"agent\"](0, config[\"game_type\"])),\n",
    "    '1' : SimpleAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "    '2' : SimpleAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "    '3' : TrainingAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "}\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_training_agent(3)\n",
    "env.set_init_game_state(None)\n",
    "\n",
    "# prefill replay memory with random actions\n",
    "if prefill_memory:\n",
    "    print('prefill replay memory')\n",
    "\n",
    "    s = env.reset()\n",
    "    while replay_memory.count() < replay_memory_capacity:\n",
    "        a = env.act(s)\n",
    "        a.append(0)\n",
    "        s1, r, d, _ = env.step(a)\n",
    "        replay_memory.add(s[3], a[3], r[3], s1[3], d)\n",
    "        s = s1 if not d else env.reset()\n",
    "\n",
    "# training loop\n",
    "try:\n",
    "    print('start training')\n",
    "    epsilon = 1.0\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "\n",
    "        # init new episode\n",
    "        ep_reward, ep_loss = 0, 0\n",
    "        d = False\n",
    "        j = -1\n",
    "        while not d:\n",
    "            j += 1\n",
    "            # select action with epsilon-greedy strategy\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a = get_numpy(policy_dqn(np.atleast_1d(s[3]))).argmax().item()\n",
    "            # perform action\n",
    "            actions = env.act(s)\n",
    "            actions.append(a)\n",
    "            s1, r, d, _ = env.step(actions)\n",
    "            # store experience in replay memory\n",
    "            replay_memory.add(s[3], a, r[3], s1[3], d)\n",
    "            # batch update\n",
    "            if replay_memory.count() >= batch_size:\n",
    "                # sample batch from replay memory\n",
    "                batch = np.array(replay_memory.sample(batch_size))\n",
    "                ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]\n",
    "                # do forward pass of batch\n",
    "                policy_dqn.optimizer.zero_grad()\n",
    "                Q = policy_dqn(ss)\n",
    "                # use target network to compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    # TODO: use target net\n",
    "                    Q1 = target_dqn(ss1)\n",
    "                # compute target for each sampled experience\n",
    "                q_targets = Q.clone()\n",
    "                for k in range(batch_size):\n",
    "                    q_targets[k, aa[k]] = rr[k] + gamma * Q1[k].max().item() * (not dd[k])\n",
    "                # update network weights\n",
    "                loss = policy_dqn.loss(Q, q_targets)\n",
    "                loss.backward()\n",
    "                policy_dqn.optimizer.step()\n",
    "                # update target network parameters from policy network parameters\n",
    "                target_dqn.update_params(policy_dqn.state_dict(), tau)\n",
    "            else:\n",
    "                loss = 0\n",
    "            # bookkeeping\n",
    "            s = s1\n",
    "            ep_reward += r[3]\n",
    "            ep_loss += loss.item()\n",
    "        # bookkeeping\n",
    "        #epsilon = epsilon\n",
    "        epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "        if (i+1) % val_freq == 0:\n",
    "            t = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "            print('%5d mean training reward: %5.2f' % (i+1, np.mean(rewards[-val_freq:])))\n",
    "            # Save file\n",
    "            t = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "            PATH = \"resources/q_agent_{}.pt\".format(t)\n",
    "            torch.save(policy_dqn.state_dict(), PATH)\n",
    "            print(\"File saved\")\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')\n",
    "\n",
    "# plot results\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "'''\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(411)\n",
    "plt.title('training rewards')\n",
    "plt.plot(range(1, num_episodes+1), rewards)\n",
    "plt.plot(moving_average(rewards))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(412)\n",
    "plt.title('training lengths')\n",
    "plt.plot(range(1, num_episodes+1), lengths)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(lengths))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(413)\n",
    "plt.title('training loss')\n",
    "plt.plot(range(1, num_episodes+1), losses)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(losses))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(414)\n",
    "plt.title('epsilon')\n",
    "plt.plot(range(1, num_episodes+1), epsilons)\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.tight_layout(); plt.show()\n",
    "'''\n",
    "## Save file\n",
    "#t = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "#PATH = \"resources/q_agent_{}.pt\".format(t)\n",
    "#torch.save(policy_dqn.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
